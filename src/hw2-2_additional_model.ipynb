{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "corpus_file = 'hw2.1_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus\n",
    "\n",
    "corpus = []\n",
    "with open(corpus_file,'r') as f:\n",
    "    for row in f:\n",
    "        row = row.replace('\\n','')\n",
    "        row = [w for w in row]\n",
    "        corpus.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use list to guarantee the embedded index for each word are same everytime\n",
    "\n",
    "words = []\n",
    "words_set = set()\n",
    "for ws in corpus:\n",
    "    for w in ws:\n",
    "        if w not in words_set:\n",
    "            words_set.add(w)\n",
    "            words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Embedding:\n",
    "\n",
    "    def __init__(self,words=None,dim=300):\n",
    "        self.word_dict = {}\n",
    "        self.word_list = []\n",
    "        self.emb_dim = dim\n",
    "        self.maxPOS = 12      # Maximum POS\n",
    "        self.addition_words = ['<PAD>','<SOS>','<EOS>','<UNK>']\n",
    "        \n",
    "        for POS in range(self.maxPOS):\n",
    "            self.addition_words.append(str(POS))\n",
    "        \n",
    "        for addition_word in self.addition_words:\n",
    "            if addition_word not in words:\n",
    "                self.word_dict[addition_word] = len(self.word_list)\n",
    "                self.word_list.append(addition_word)\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in self.word_dict:\n",
    "                self.word_dict[word] = len(self.word_list)\n",
    "                self.word_list.append(word)\n",
    "                \n",
    "        self.vectors = torch.nn.init.uniform_(\n",
    "                torch.empty(len(self.word_dict),dim))\n",
    "        \n",
    "    def to_index(self, word):\n",
    "        # single word tokenize\n",
    "        if word not in self.word_dict:\n",
    "            return self.word_dict['<UNK>']\n",
    "        \n",
    "        return self.word_dict[word]\n",
    "        \n",
    "    def tokenize(self, words):\n",
    "        # whole sentence tokenize\n",
    "        return [self.to_index(w) for w in words]\n",
    "    \n",
    "    def to_word(self, idx):\n",
    "        \n",
    "        return self.word_list[idx]\n",
    "        \n",
    "    def unTokenize(self,ids):\n",
    "        \n",
    "        return [self.to_word(idx) for idx in ids]\n",
    "        \n",
    "    def get_vocabulary_size(self):\n",
    "        return self.vectors.shape[0]\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self.vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish embedder to tokenize\n",
    "embedder = Embedding(words=words,dim=300)\n",
    "\n",
    "PAD = embedder.to_index('<PAD>')\n",
    "SOS = embedder.to_index('<SOS>')\n",
    "EOS = embedder.to_index('<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_set = []\n",
    "two_hint_ratio = 0.0\n",
    "\n",
    "for former,latter in zip(corpus[:-1],corpus[1:]):\n",
    "    \n",
    "    n = len(latter)\n",
    "    addition_tokens = []\n",
    "    \n",
    "    selected_idice = random.sample(list(range(min(n,embedder.maxPOS))), k=1)\n",
    "    selected_idx = selected_idice[0]\n",
    "    \n",
    "    a = random.randint(a=0,b=selected_idx+1)\n",
    "    b = selected_idx + 1 - a\n",
    "    \n",
    "    addition_tokens.append(str(a))\n",
    "    addition_tokens.append(str(b))\n",
    "    addition_tokens.append(latter[selected_idx])\n",
    "    \n",
    "    \n",
    "    former = ['<SOS>'] + former + ['<EOS>'] + addition_tokens\n",
    "    latter = ['<SOS>'] + latter + ['<EOS>']\n",
    "    \n",
    "\n",
    "    \n",
    "    all_set.append((former,latter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741714 593371 148343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set,valid_set = train_test_split(all_set,test_size=0.2,random_state=42)\n",
    "\n",
    "print(len(all_set),len(train_set),len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['<SOS>',\n",
       "   '心',\n",
       "   '疼',\n",
       "   '你',\n",
       "   '还',\n",
       "   '没',\n",
       "   '挣',\n",
       "   '脱',\n",
       "   '思',\n",
       "   '念',\n",
       "   '的',\n",
       "   '囚',\n",
       "   '禁',\n",
       "   '<EOS>',\n",
       "   '1',\n",
       "   '1',\n",
       "   '在'],\n",
       "  ['<SOS>',\n",
       "   '他',\n",
       "   '在',\n",
       "   '你',\n",
       "   '一',\n",
       "   '段',\n",
       "   '难',\n",
       "   '忘',\n",
       "   '远',\n",
       "   '行',\n",
       "   '最',\n",
       "   '后',\n",
       "   '却',\n",
       "   '离',\n",
       "   '去',\n",
       "   '<EOS>']),\n",
       " (['<SOS>',\n",
       "   '他',\n",
       "   '在',\n",
       "   '你',\n",
       "   '一',\n",
       "   '段',\n",
       "   '难',\n",
       "   '忘',\n",
       "   '远',\n",
       "   '行',\n",
       "   '最',\n",
       "   '后',\n",
       "   '却',\n",
       "   '离',\n",
       "   '去',\n",
       "   '<EOS>',\n",
       "   '1',\n",
       "   '7',\n",
       "   '这'],\n",
       "  ['<SOS>', '你', '无', '力', '依', '靠', '在', '我', '这', '里', '<EOS>']),\n",
       " (['<SOS>',\n",
       "   '你',\n",
       "   '无',\n",
       "   '力',\n",
       "   '依',\n",
       "   '靠',\n",
       "   '在',\n",
       "   '我',\n",
       "   '这',\n",
       "   '里',\n",
       "   '<EOS>',\n",
       "   '2',\n",
       "   '0',\n",
       "   '着'],\n",
       "  ['<SOS>', '隔', '着', '刚', '被', '雨', '淋', '湿', '的', '玻', '璃', '<EOS>']),\n",
       " (['<SOS>',\n",
       "   '隔',\n",
       "   '着',\n",
       "   '刚',\n",
       "   '被',\n",
       "   '雨',\n",
       "   '淋',\n",
       "   '湿',\n",
       "   '的',\n",
       "   '玻',\n",
       "   '璃',\n",
       "   '<EOS>',\n",
       "   '2',\n",
       "   '3',\n",
       "   '到'],\n",
       "  ['<SOS>', '你', '问', '了', '我', '到', '底', '爱', '在', '哪', '里', '<EOS>']),\n",
       " (['<SOS>',\n",
       "   '你',\n",
       "   '问',\n",
       "   '了',\n",
       "   '我',\n",
       "   '到',\n",
       "   '底',\n",
       "   '爱',\n",
       "   '在',\n",
       "   '哪',\n",
       "   '里',\n",
       "   '<EOS>',\n",
       "   '9',\n",
       "   '0',\n",
       "   '剩'],\n",
       "  ['<SOS>',\n",
       "   '你',\n",
       "   '最',\n",
       "   '想',\n",
       "   '去',\n",
       "   '的',\n",
       "   '目',\n",
       "   '的',\n",
       "   '地',\n",
       "   '剩',\n",
       "   '多',\n",
       "   '少',\n",
       "   '公',\n",
       "   '里',\n",
       "   '<EOS>'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of turple : [(x0,y0),(x1,y1),(x2,y2),....]\n",
    "all_set[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_data_len = max([len(data[0]) for data in datas])\n",
    "        max_label_len = max([len(data[1]) for data in datas])\n",
    "        \n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        len_x = []\n",
    "        len_y = []\n",
    "        batch_y_ = []\n",
    "        \n",
    "        \n",
    "        for data,label in datas:\n",
    "            \n",
    "            len_x.append(len(data))\n",
    "            len_y.append(len(label))\n",
    "            \n",
    "            # Tokenize\n",
    "            pad_data = embedder.tokenize(data)\n",
    "            pad_label = embedder.tokenize(label)\n",
    "            \n",
    "            # Padding data and label\n",
    "            if len(data) < max_data_len:\n",
    "                pad_data.extend([PAD] * (max_data_len-len(data)))\n",
    "            if len(label) < max_label_len:\n",
    "                pad_label.extend([PAD] * (max_label_len-len(label)))\n",
    "                \n",
    "                \n",
    "            batch_x.append(pad_data)\n",
    "            batch_y.append(pad_label)\n",
    "            \n",
    "            # generate y_\n",
    "            focus_designate = [PAD] * len(pad_label)\n",
    "            focus_designate[0] = SOS\n",
    "            focus_designate[pad_label.index(EOS)] = EOS\n",
    "            \n",
    "            idx = int(data[data.index('<EOS>')+1]) + int(data[data.index('<EOS>')+2])\n",
    "            focus_designate[idx] = pad_label[idx]\n",
    "            \n",
    "            batch_y_.append(focus_designate)\n",
    "            \n",
    "            \n",
    "        return torch.LongTensor(batch_x), torch.LongTensor(len_x), torch.LongTensor(batch_y), len_y, torch.LongTensor(batch_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence lenght: tensor([11, 14, 15, 18]) [8, 7, 8, 12] \n",
      "\n",
      "['<SOS>', '我', '没', '有', '六', '尺', '高', '<EOS>', '3', '0', '会', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '我', '却', '会', '待', '你', '好', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '<PAD>', '<PAD>', '会', '<PAD>', '<PAD>', '<PAD>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] \n",
      "\n",
      "['<SOS>', '替', '我', '解', '开', '心', '中', '的', '孤', '单', '<EOS>', '1', '3', '白', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '是', '谁', '明', '白', '我', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '<PAD>', '<PAD>', '<PAD>', '白', '<PAD>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] \n",
      "\n",
      "['<SOS>', '爱', '是', '不', '是', '不', '开', '口', '才', '珍', '贵', '<EOS>', '0', '3', '我', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '再', '给', '我', '两', '分', '钟', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '<PAD>', '<PAD>', '我', '<PAD>', '<PAD>', '<PAD>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] \n",
      "\n",
      "['<SOS>', '准', '备', '好', '冬', '天', '将', '你', '的', '一', '切', '都', '遗', '忘', '<EOS>', '0', '1', '我']\n",
      "['<SOS>', '我', '的', '爱', '情', '已', '折', '断', '了', '翅', '膀', '<EOS>']\n",
      "['<SOS>', '我', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EOS>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Validate~~~~\n",
    "\n",
    "dataset = SentDataset(train_set)\n",
    "dataloader = DataLoader(dataset=dataset,\n",
    "                        batch_size=4,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=dataset.collate_fn,\n",
    "                        num_workers=0)\n",
    "for x,x_len,y,y_len,y_ in dataloader:\n",
    "    print('Sentence lenght:',x_len,y_len,'\\n')\n",
    "    \n",
    "    for xi,yi,y_i in zip(x,y,y_):      \n",
    "        \n",
    "        print(embedder.unTokenize(xi))\n",
    "        print(embedder.unTokenize(yi))\n",
    "        print(embedder.unTokenize(y_i),'\\n')\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import  pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, output_size):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedder.get_dim())\n",
    "        self.embedding.weight = nn.Parameter(embedder.vectors)\n",
    "        self.gru = nn.GRU(embedding_size, output_size,batch_first=True, bias=False)\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        \n",
    "        # Sort mini-batch by input_lengths\n",
    "        sorted_input_lengths, indices = torch.sort(input_lengths,descending=True)\n",
    "        _, desorted_indices = torch.sort(indices, descending=False)\n",
    "        input_seqs = input_seqs[indices]\n",
    "        \n",
    "        # Encoder work\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = pack_padded_sequence(embedded, sorted_input_lengths.cpu().numpy(), batch_first=True)\n",
    "        packed_outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = pad_packed_sequence(packed_outputs,batch_first=True)\n",
    "        \n",
    "        # Desort mini-batch\n",
    "        outputs = outputs[desorted_indices]\n",
    "        hidden = hidden[:,desorted_indices]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, teacher_forcing_ratio=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(embedder.get_vocabulary_size(),embedder.get_dim()) # Unused\n",
    "        self.embedding.weight = nn.Parameter(embedder.vectors)\n",
    "        self.cell = nn.GRUCell(embedder.get_dim(), hidden_size, bias=False)\n",
    "        self.clf = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        \n",
    "        if hidden_size == embedder.vectors.T.shape[0]:\n",
    "            self.clf.weight = nn.Parameter(embedder.vectors)\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)  # work with NLLLoss\n",
    "\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward_step(self, inputs, hidden):\n",
    "        \n",
    "        # Unused\n",
    "        embedded = self.embedding(inputs)\n",
    "        # For research : all x to 0\n",
    "        embedded = torch.zeros_like(embedded)\n",
    "        \n",
    "        hidden = self.cell(embedded, hidden) # [B,Hidden_dim]\n",
    "        clf_output = self.clf(hidden) # [B,Output_dim]\n",
    "        output = self.log_softmax(clf_output)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, context_vector, target_vars, target_lengths):\n",
    "\n",
    "        batch_size = context_vector.shape[1]\n",
    "        \n",
    "        decoder_input = torch.LongTensor([SOS] * batch_size).to(device)\n",
    "        decoder_hidden = context_vector.squeeze(0)\n",
    "\n",
    "        if target_lengths is None:\n",
    "            max_target_length = 50\n",
    "        else:\n",
    "            max_target_length = max(target_lengths)\n",
    "        decoder_outputs = []\n",
    "        decoder_hiddens = []\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        \n",
    "        for t in range(max_target_length):    \n",
    "            \n",
    "            decoder_outputs_on_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_outputs_on_t)\n",
    "            decoder_hiddens.append(decoder_hidden)\n",
    "            \n",
    "            # Take input for next GRU iteration\n",
    "            if use_teacher_forcing :\n",
    "                decoder_input = target_vars[:,t]\n",
    "            else:\n",
    "                decoder_input = decoder_outputs_on_t.argmax(-1)\n",
    "            \n",
    "            # Early Stop when all predict <EOS> \n",
    "            if torch.all(decoder_input==EOS) and target_lengths is None and self.train() == False:\n",
    "                break\n",
    "            \n",
    "        # Stack output of each word at dimension 2\n",
    "        decoder_outputs = torch.stack(decoder_outputs,dim=2)\n",
    "        # Stack hidden of each timestep at dimension 1\n",
    "        decoder_hiddens = torch.stack(decoder_hiddens,dim=1)\n",
    "        \n",
    "        return decoder_outputs, decoder_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, input_seqs, input_lengths, target_seqs=None, target_lengths=None):\n",
    "        outputs, hidden = encoder(input_seqs, input_lengths)\n",
    "        outputs, hiddens = decoder(hidden, target_seqs, target_lengths)\n",
    "        return outputs,hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6575, 300)\n",
       "    (gru): GRU(300, 128, bias=False, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(6575, 300)\n",
       "    (cell): GRUCell(300, 128, bias=False)\n",
       "    (clf): Linear(in_features=128, out_features=6575, bias=False)\n",
       "    (log_softmax): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "context_dim = 128\n",
    "\n",
    "encoder = Encoder(embedder.get_vocabulary_size(),embedder.get_dim(),output_size=context_dim)\n",
    "decoder = Decoder(context_dim,embedder.get_vocabulary_size(),0.5)\n",
    "model = Seq2Seq(encoder,decoder)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_epoch(epoch,dataset,isTraining):\n",
    "    \n",
    "    if isTraining:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=0)\n",
    "    \n",
    "    if isTraining:\n",
    "        desc='Train {}'\n",
    "    else:\n",
    "        desc='Valid {}'\n",
    "    \n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader),desc=desc.format(epoch))\n",
    "    \n",
    "    loss=0\n",
    "    acc = 0\n",
    "    \n",
    "    for i,(x,x_len,y,y_len,y_) in trange:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_ = y_.to(device)\n",
    "        \n",
    "        # outputs : [b,emb,s] , hiddens : [b,s,hidden]\n",
    "        outputs,hiddens = model(x,x_len,y,y_len)\n",
    "        \n",
    "        idx = y_>2\n",
    "        tf_map = y_[idx] == outputs.argmax(1)[idx]\n",
    "        batch_acc = tf_map.sum().cpu().float().numpy()/len(tf_map)\n",
    "        acc += batch_acc\n",
    "        \n",
    "        batch_loss_all = criterion(outputs, y)\n",
    "        batch_loss_designated = criterion(outputs, y_)\n",
    "        batch_loss = (1-focus_ratio)*batch_loss_all + focus_ratio*batch_loss_designated\n",
    "        \n",
    "        if isTraining:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss += batch_loss.item()\n",
    "        \n",
    "        trange.set_postfix({'loss':loss/(i+1),'accuracy':acc/(i+1)})\n",
    "        \n",
    "        if isTraining:\n",
    "            history_loss['train'].append(batch_loss.item())\n",
    "            history_acc['train'].append(batch_acc)\n",
    "        else:\n",
    "            history_loss['valid'].append(batch_loss.item())\n",
    "            history_acc['valid'].append(batch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "import os\n",
    "\n",
    "dataset_all = SentDataset(all_set)\n",
    "dataset_train = SentDataset(train_set)\n",
    "dataset_valid = SentDataset(valid_set)\n",
    "\n",
    "criterion = torch.nn.NLLLoss(ignore_index=PAD, size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "max_epoch = 20\n",
    "focus_ratio = 0.5\n",
    "\n",
    "history_loss = {'train':[],'valid':[]}\n",
    "history_acc = {'train':[],'valid':[]}\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    # Training\n",
    "    run_epoch(epoch,dataset=dataset_train,isTraining=True)\n",
    "    \n",
    "    # Validation\n",
    "    run_epoch(epoch,dataset=dataset_valid,isTraining=False)\n",
    "    \n",
    "    # Saving\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    torch.save(model.state_dict(), 'model/model.pkl.{}'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot (Loss and acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "modes = ['train', 'valid']\n",
    "recs = [history_loss, history_acc]\n",
    "names = ['Loss', 'Accuracy']\n",
    "\n",
    "values = []\n",
    "for mode in modes:\n",
    "    v = []\n",
    "    for rec in recs:\n",
    "        v.append(rec[mode])\n",
    "    values.append(v)\n",
    "\n",
    "plt.figure(figsize=(32, 4))\n",
    "plt.subplots_adjust(left=0.02, right=0.999)\n",
    "for r, name in enumerate(names):\n",
    "    plt.subplot(1, len(recs), r+1)\n",
    "    for m in range(len(modes)):\n",
    "        plt.plot(values[m][r])\n",
    "    plt.title(name)\n",
    "    plt.legend(modes)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.show()\n",
    "#plt.savefig('figure.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test Data\n",
    "### Define test data dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        max_data_len = max([len(data) for data in datas])\n",
    "        batch_x = []\n",
    "        len_x = []\n",
    "        \n",
    "        for data in datas:\n",
    "            len_x.append(len(data))\n",
    "            pad_data = [embedder.to_index(w) for w in data]\n",
    "            if len(data) < max_data_len:\n",
    "                pad_data.extend([PAD] * (max_data_len-len(data)))\n",
    "            batch_x.append(pad_data)\n",
    "\n",
    "        return torch.LongTensor(batch_x), torch.LongTensor(len_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6575, 300)\n",
       "    (gru): GRU(300, 128, bias=False, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(6575, 300)\n",
       "    (cell): GRUCell(300, 128, bias=False)\n",
       "    (clf): Linear(in_features=128, out_features=6575, bias=False)\n",
       "    (log_softmax): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_pkl = 'pre-train/model.pkl.2-2-additional'\n",
    "model.load_state_dict(torch.load(path_pkl))\n",
    "model.decoder.teacher_forcing_ratio = 0.0\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def Deconstruction(model,x,x_len):\n",
    "    \n",
    "    # Encoder \n",
    "    encoder_hiddens, context = model.encoder(x,x_len)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_hidden = context.squeeze(0)\n",
    "    \n",
    "    # Collection signal\n",
    "    decoder_outputs = []\n",
    "    decoder_hiddens = []\n",
    "    decoder_resetGates = []\n",
    "    decoder_updateGates = []\n",
    "    decoder_newGates = []\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # GRU Cell\n",
    "        gru = model.decoder.cell\n",
    "        \n",
    "        U_h = F.linear(decoder_hidden, gru.weight_hh)\n",
    "        Ur_h, Uz_h, Un_h = U_h.chunk(3, 1)\n",
    "        reset_gate = torch.sigmoid(Ur_h)\n",
    "        update_gate = torch.sigmoid(Uz_h)\n",
    "        new_gate = torch.tanh(reset_gate * Un_h)\n",
    "        decoder_hidden = new_gate + update_gate * (decoder_hidden - new_gate)\n",
    "        \n",
    "        # Classifier\n",
    "        clf_output = model.decoder.clf(decoder_hidden)\n",
    "        decoder_output = model.decoder.log_softmax(clf_output)\n",
    "        \n",
    "        decoder_resetGates.append(reset_gate)\n",
    "        decoder_updateGates.append(update_gate)\n",
    "        decoder_newGates.append(new_gate)\n",
    "        decoder_outputs.append(decoder_output)            \n",
    "        decoder_hiddens.append(decoder_hidden)\n",
    "                                                   \n",
    "        if torch.all(decoder_output.argmax(-1)==EOS) == True:\n",
    "            break\n",
    "            \n",
    "    outputs = torch.stack(decoder_outputs,dim=2)             # (b,6xxx,s)\n",
    "    \n",
    "    gru_info = {\n",
    "        'hiddens':torch.stack(decoder_hiddens,dim=2),             # (b,128,s)\n",
    "        'resetgates':torch.stack(decoder_resetGates,dim=2),       # (b,128,s)\n",
    "        'updategates':torch.stack(decoder_updateGates,dim=2),     # (b,128,s)\n",
    "        'newgates':torch.stack(decoder_newGates,dim=2)            # (b,128,s)\n",
    "    }\n",
    "    \n",
    "    return outputs, gru_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate certain condition valid datas ( by designated word / position filter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', '受', '不', '了', '看', '见', '你', '背', '影', '来', '到', '<EOS>', '1', '3', '乐']\n",
      "['<SOS>', '合', '明', '明', '无', '余', '地', '再', '过', '问', '<EOS>', '0', '5', '乐']\n",
      "['<SOS>', '春', '风', '扬', '起', '你', '我', '的', '离', '别', '<EOS>', '2', '2', '乐']\n",
      "['<SOS>', '旧', '爱', '再', '莫', '忆', '心', '中', '<EOS>', '6', '4', '乐']\n",
      "['<SOS>', '告', '诉', '你', '我', '心', '里', '一', '直', '都', '懂', '你', '<EOS>', '6', '1', '乐']\n",
      "['<SOS>', '任', '旧', '日', '万', '念', '俱', '灰', '也', '经', '过', '<EOS>', '1', '0', '乐']\n",
      "['<SOS>', '错', '过', '了', '多', '少', '个', '路', '口', '<EOS>', '3', '0', '乐']\n",
      "['<SOS>', '饥', '吞', '毡', '渴', '饮', '雪', '<EOS>', '1', '9', '乐']\n",
      "['<SOS>', '讲', '分', '开', '可', '否', '不', '再', '用', '憾', '事', '的', '口', '吻', '<EOS>', '3', '2', '乐']\n",
      "['<SOS>', '愛', '<EOS>', '5', '2', '乐']\n",
      "['<SOS>', '期', '望', '带', '来', '失', '望', '的', '恶', '性', '循', '环', '<EOS>', '6', '4', '乐']\n",
      "['<SOS>', '风', '起', '云', '过', '雨', '打', '湿', '过', '<EOS>', '8', '2', '乐']\n",
      "['<SOS>', '水', '没', '了', '<EOS>', '8', '1', '乐']\n",
      "['<SOS>', '放', '过', '你', '自', '己', '吧', '勇', '敢', '一', '点', '面', '对', '<EOS>', '0', '1', '乐']\n",
      "['<SOS>', '让', '惊', '慌', '的', '泪', '水', '不', '再', '无', '处', '可', '躲', '<EOS>', '5', '2', '乐']\n",
      "['<SOS>', '是', '我', '唯', '一', '的', '爱', '<EOS>', '3', '0', '乐']\n"
     ]
    }
   ],
   "source": [
    "certain_set = []\n",
    "\n",
    "designated_word = '乐'\n",
    "\n",
    "for sent in random.sample(corpus, k=16):\n",
    "    \n",
    "    designated_POS = random.randint(a=1,b=10)\n",
    "    designated_POS1 = random.randint(a=0,b=designated_POS)\n",
    "    designated_POS2 = designated_POS - designated_POS1\n",
    "    \n",
    "    control_signal = [str(designated_POS1) , str(designated_POS2) , designated_word]\n",
    "    \n",
    "    data = ['<SOS>'] + sent + ['<EOS>'] + control_signal\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    certain_set.append(data)\n",
    "    \n",
    "dataset_certain = TestDataset(certain_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction certain condition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0648c6c9a34e45af9f87a3edd1573b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=dataset_certain,\n",
    "                        batch_size=128,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=dataset_certain.collate_fn,\n",
    "                        num_workers=0)\n",
    "\n",
    "predictions = []\n",
    "trange = tqdm(dataloader, total=len(dataloader))\n",
    "\n",
    "for x,x_len in trange:\n",
    "    \n",
    "    x = x.to(device)\n",
    "    \n",
    "    outputs,gru_info = Deconstruction(model,x,x_len)\n",
    "    \n",
    "    for pred in outputs.cpu().detach().numpy().argmax(1):\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\t<SOS>受不了看见你背影来到<EOS>13乐\n",
      "pred:\t<SOS>我的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>合明明无余地再过问<EOS>05乐\n",
      "pred:\t<SOS>我你你快乐<EOS>\n",
      "\n",
      "input:\t<SOS>春风扬起你我的离别<EOS>22乐\n",
      "pred:\t<SOS>我是快乐<EOS>\n",
      "\n",
      "input:\t<SOS>旧爱再莫忆心中<EOS>64乐\n",
      "pred:\t<SOS>我是的的的的我的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>告诉你我心里一直都懂你<EOS>61乐\n",
      "pred:\t<SOS>我你你你的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>任旧日万念俱灰也经过<EOS>10乐\n",
      "pred:\t<SOS>乐<EOS>\n",
      "\n",
      "input:\t<SOS>错过了多少个路口<EOS>30乐\n",
      "pred:\t<SOS>我快乐<EOS>\n",
      "\n",
      "input:\t<SOS>饥吞毡渴饮雪<EOS>19乐\n",
      "pred:\t<SOS>我是的的的的的的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>讲分开可否不再用憾事的口吻<EOS>32乐\n",
      "pred:\t<SOS>我是的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>愛<EOS>52乐\n",
      "pred:\t<SOS>我我我我的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>期望带来失望的恶性循环<EOS>64乐\n",
      "pred:\t<SOS>我是的的的的的的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>风起云过雨打湿过<EOS>82乐\n",
      "pred:\t<SOS>我你你你你你我的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>水没了<EOS>81乐\n",
      "pred:\t<SOS>我是的的的的的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>放过你自己吧勇敢一点面对<EOS>01乐\n",
      "pred:\t<SOS>乐<EOS>\n",
      "\n",
      "input:\t<SOS>让惊慌的泪水不再无处可躲<EOS>52乐\n",
      "pred:\t<SOS>我是你我的快乐<EOS>\n",
      "\n",
      "input:\t<SOS>是我唯一的爱<EOS>30乐\n",
      "pred:\t<SOS>我快乐<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process multi-EOS tokens\n",
    "predictions_set = []\n",
    "for p in predictions:\n",
    "    p = list(p)\n",
    "    if EOS in p:\n",
    "        p = p[:p.index(EOS)+1]\n",
    "    else:\n",
    "        p.append(EOS)\n",
    "    predictions_set.append(embedder.unTokenize(p))\n",
    "    \n",
    "\n",
    "for sent_id in range(len(certain_set)):\n",
    "    sent_in = ''.join(certain_set[sent_id])\n",
    "    sent_pred = ''.join(predictions_set[sent_id])\n",
    "    print('input:\\t{}\\npred:\\t{}\\n'.format(sent_in,sent_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updategates = gru_info['updategates'].mean(0).detach().cpu().numpy()\n",
    "\n",
    "resetgates = gru_info['resetgates'].mean(0).detach().cpu().numpy()\n",
    "\n",
    "newgates = gru_info['newgates'].mean(0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "output_len = updategates.shape[1]\n",
    "\n",
    "updategates.shape , resetgates.shape , newgates.shape\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Update Gates\n",
    "\n",
    "plt.close('all') \n",
    "x = np.arange(output_len)\n",
    "\n",
    "fig , ax = plt.subplots()\n",
    "\n",
    "for i,c in enumerate(updategates):\n",
    "    \n",
    "    plt.plot(x, c,label='cell {}'.format(i))\n",
    "    \n",
    "    if (i+1)%128 == 0:\n",
    "        #leg = ax.legend(loc='upper right', shadow=True)\n",
    "        plt.xlim((-0.5, output_len+1))\n",
    "        if i < 127:\n",
    "            fig , ax = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reset Gates\n",
    "\n",
    "plt.close('all') \n",
    "x = np.arange(output_len)\n",
    "\n",
    "fig , ax = plt.subplots()\n",
    "\n",
    "for i,c in enumerate(resetgates):\n",
    "    \n",
    "    plt.plot(x, c,label='cell {}'.format(i))\n",
    "    if (i+1)%32 == 0:\n",
    "        leg = ax.legend(loc='upper right', shadow=True)\n",
    "        plt.xlim((-0.5, output_len+1))\n",
    "        if i < 127:\n",
    "            fig , ax = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# New Gates\n",
    "\n",
    "plt.close('all') \n",
    "x = np.arange(output_len)\n",
    "\n",
    "fig , ax = plt.subplots()\n",
    "\n",
    "for i,c in enumerate(newgates):\n",
    "    \n",
    "    plt.plot(x, c,label='cell {}'.format(i))\n",
    "    if (i+1)%128 == 0:\n",
    "        #leg = ax.legend(loc='upper right', shadow=True)\n",
    "        plt.xlim((-0.5, output_len+1))\n",
    "        if i < 127:\n",
    "            fig , ax = plt.subplots()\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_cells = [18,19,42,47,48,50,57,60,62,67,68,74,71,82,88,90,91,92,93,96,97,100,116,125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all') \n",
    "x = np.arange(output_len)\n",
    "\n",
    "for i,c in enumerate(updategates):\n",
    "    \n",
    "    if i in special_cells:\n",
    "        plt.plot(x, c,label='cell {}'.format(i))\n",
    "plt.xlim((-0.5, output_len+1))        \n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all') \n",
    "x = np.arange(output_len)\n",
    "\n",
    "for i,c in enumerate(resetgates):\n",
    "    if i in special_cells:\n",
    "        plt.plot(x, c,label='cell {}'.format(i))\n",
    "plt.xlim((-0.5, output_len+1))        \n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
