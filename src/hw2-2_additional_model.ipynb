{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corpus_file = '../hw2.1_corpus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Corpus\n",
    "\n",
    "corpus = []\n",
    "with open(corpus_file,'r') as f:\n",
    "    for row in f:\n",
    "        row = row.replace('\\n','')\n",
    "        row = [w for w in row]\n",
    "        corpus.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use list to guarantee the embedded index for each word are same everytime\n",
    "\n",
    "words = []\n",
    "words_set = set()\n",
    "for ws in corpus:\n",
    "    for w in ws:\n",
    "        if w not in words_set:\n",
    "            words_set.add(w)\n",
    "            words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Embedding:\n",
    "\n",
    "    def __init__(self,words=None,dim=300):\n",
    "        self.word_dict = {}\n",
    "        self.word_list = []\n",
    "        self.emb_dim = dim\n",
    "        self.maxPOS = 12      # Maximum POS\n",
    "        self.addition_words = ['<PAD>','<SOS>','<EOS>','<UNK>']\n",
    "        \n",
    "        for POS in range(self.maxPOS):\n",
    "            self.addition_words.append(str(POS))\n",
    "        \n",
    "        for addition_word in self.addition_words:\n",
    "            if addition_word not in words:\n",
    "                self.word_dict[addition_word] = len(self.word_list)\n",
    "                self.word_list.append(addition_word)\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in self.word_dict:\n",
    "                self.word_dict[word] = len(self.word_list)\n",
    "                self.word_list.append(word)\n",
    "                \n",
    "        self.vectors = torch.nn.init.uniform_(\n",
    "                torch.empty(len(self.word_dict),dim))\n",
    "        \n",
    "    def to_index(self, word):\n",
    "        # single word tokenize\n",
    "        if word not in self.word_dict:\n",
    "            return self.word_dict['<UNK>']\n",
    "        \n",
    "        return self.word_dict[word]\n",
    "        \n",
    "    def tokenize(self, words):\n",
    "        # whole sentence tokenize\n",
    "        return [self.to_index(w) for w in words]\n",
    "    \n",
    "    def to_word(self, idx):\n",
    "        \n",
    "        return self.word_list[idx]\n",
    "        \n",
    "    def unTokenize(self,ids):\n",
    "        \n",
    "        return [self.to_word(idx) for idx in ids]\n",
    "        \n",
    "    def get_vocabulary_size(self):\n",
    "        return self.vectors.shape[0]\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self.vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish embedder to tokenize\n",
    "embedder = Embedding(words=words,dim=300)\n",
    "\n",
    "PAD = embedder.to_index('<PAD>')\n",
    "SOS = embedder.to_index('<SOS>')\n",
    "EOS = embedder.to_index('<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0. 53060. 33876. 24074. 18301. 13818. 14125.  7228.  5013.  3371.\n",
      "   2188.  1439.   944.]\n",
      " [53167. 33951. 24179. 18326. 13813. 14373.  7309.  5108.  3298.  2226.\n",
      "   1414.   934.     0.]\n",
      " [33656. 24099. 18229. 13667.     0.  7505.  5008.  3375.  2225.  1480.\n",
      "    921.     0.     0.]\n",
      " [24445. 18210. 13826. 14295.  7319.  5192.  3356.  2262.  1437.   919.\n",
      "      0.     0.     0.]\n",
      " [18264. 13736.     0.  7336.  5142.  3450.  2280.  1409.   906.     0.\n",
      "      0.     0.     0.]\n",
      " [13800. 14103.  7429.  5081.  3443.  2217.  1447.   873.     0.     0.\n",
      "      0.     0.     0.]\n",
      " [14332.  7519.  4892.  3346.  2291.  1415.   915.     0.     0.     0.\n",
      "      0.     0.     0.]\n",
      " [ 7421.  4993.  3497.  2170.  1459.   880.     0.     0.     0.     0.\n",
      "      0.     0.     0.]\n",
      " [ 5130.  3307.  2342.  1382.   909.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.]\n",
      " [ 3375.  2219.  1400.   887.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.]\n",
      " [ 2230.  1413.   868.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.]\n",
      " [ 1471.   903.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.]\n",
      " [  871.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "      0.     0.     0.]]\n"
     ]
    }
   ],
   "source": [
    "all_set = []\n",
    "counting_table = np.zeros((embedder.maxPOS+1,embedder.maxPOS+1))\n",
    "\n",
    "for former,latter in zip(corpus[:-1],corpus[1:]):\n",
    "    \n",
    "    n = len(latter)\n",
    "    \n",
    "    addition_tokens = []\n",
    "    \n",
    "    former = ['<SOS>'] + former + ['<EOS>'] \n",
    "    latter = ['<SOS>'] + latter + ['<EOS>']\n",
    "    \n",
    "    selected_idx = random.randint(a=1,b=min(n,embedder.maxPOS))\n",
    "    \n",
    "    while 1:\n",
    "        a = random.randint(a=0,b=selected_idx)\n",
    "        b = selected_idx - a\n",
    "        \n",
    "        if (a,b) not in [(4,2),(2,4)]:\n",
    "            break\n",
    "        \n",
    "    counting_table[a,b] += 1\n",
    "    \n",
    "    addition_tokens.append(latter[selected_idx])\n",
    "    if a > 0:    \n",
    "        addition_tokens.append(str(a))\n",
    "    if b > 0:\n",
    "        addition_tokens.append(str(b))\n",
    "    \n",
    "    former = former + addition_tokens\n",
    "    \n",
    "    all_set.append((former,latter))\n",
    "    \n",
    "print(counting_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741714 593371 148343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set,valid_set = train_test_split(all_set,test_size=0.2,random_state=42)\n",
    "\n",
    "print(len(all_set),len(train_set),len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['<SOS>',\n",
       "   '心',\n",
       "   '疼',\n",
       "   '你',\n",
       "   '还',\n",
       "   '没',\n",
       "   '挣',\n",
       "   '脱',\n",
       "   '思',\n",
       "   '念',\n",
       "   '的',\n",
       "   '囚',\n",
       "   '禁',\n",
       "   '<EOS>',\n",
       "   '在',\n",
       "   '1',\n",
       "   '1'],\n",
       "  ['<SOS>',\n",
       "   '他',\n",
       "   '在',\n",
       "   '你',\n",
       "   '一',\n",
       "   '段',\n",
       "   '难',\n",
       "   '忘',\n",
       "   '远',\n",
       "   '行',\n",
       "   '最',\n",
       "   '后',\n",
       "   '却',\n",
       "   '离',\n",
       "   '去',\n",
       "   '<EOS>']),\n",
       " (['<SOS>',\n",
       "   '他',\n",
       "   '在',\n",
       "   '你',\n",
       "   '一',\n",
       "   '段',\n",
       "   '难',\n",
       "   '忘',\n",
       "   '远',\n",
       "   '行',\n",
       "   '最',\n",
       "   '后',\n",
       "   '却',\n",
       "   '离',\n",
       "   '去',\n",
       "   '<EOS>',\n",
       "   '依',\n",
       "   '3',\n",
       "   '1'],\n",
       "  ['<SOS>', '你', '无', '力', '依', '靠', '在', '我', '这', '里', '<EOS>']),\n",
       " (['<SOS>',\n",
       "   '你',\n",
       "   '无',\n",
       "   '力',\n",
       "   '依',\n",
       "   '靠',\n",
       "   '在',\n",
       "   '我',\n",
       "   '这',\n",
       "   '里',\n",
       "   '<EOS>',\n",
       "   '被',\n",
       "   '1',\n",
       "   '3'],\n",
       "  ['<SOS>', '隔', '着', '刚', '被', '雨', '淋', '湿', '的', '玻', '璃', '<EOS>']),\n",
       " (['<SOS>',\n",
       "   '隔',\n",
       "   '着',\n",
       "   '刚',\n",
       "   '被',\n",
       "   '雨',\n",
       "   '淋',\n",
       "   '湿',\n",
       "   '的',\n",
       "   '玻',\n",
       "   '璃',\n",
       "   '<EOS>',\n",
       "   '问',\n",
       "   '2'],\n",
       "  ['<SOS>', '你', '问', '了', '我', '到', '底', '爱', '在', '哪', '里', '<EOS>']),\n",
       " (['<SOS>',\n",
       "   '你',\n",
       "   '问',\n",
       "   '了',\n",
       "   '我',\n",
       "   '到',\n",
       "   '底',\n",
       "   '爱',\n",
       "   '在',\n",
       "   '哪',\n",
       "   '里',\n",
       "   '<EOS>',\n",
       "   '的',\n",
       "   '2',\n",
       "   '3'],\n",
       "  ['<SOS>',\n",
       "   '你',\n",
       "   '最',\n",
       "   '想',\n",
       "   '去',\n",
       "   '的',\n",
       "   '目',\n",
       "   '的',\n",
       "   '地',\n",
       "   '剩',\n",
       "   '多',\n",
       "   '少',\n",
       "   '公',\n",
       "   '里',\n",
       "   '<EOS>'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of turple : [(x0,y0),(x1,y1),(x2,y2),....]\n",
    "all_set[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_data_len = max([len(data[0]) for data in datas])\n",
    "        max_label_len = max([len(data[1]) for data in datas])\n",
    "        \n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        len_x = []\n",
    "        len_y = []\n",
    "        batch_y_ = []\n",
    "        \n",
    "        \n",
    "        for data,label in datas:\n",
    "            \n",
    "            len_x.append(len(data))\n",
    "            len_y.append(len(label))\n",
    "            \n",
    "            # Tokenize\n",
    "            pad_data = embedder.tokenize(data)\n",
    "            pad_label = embedder.tokenize(label)\n",
    "            \n",
    "            # Padding data and label\n",
    "            if len(data) < max_data_len:\n",
    "                pad_data.extend([PAD] * (max_data_len-len(data)))\n",
    "            if len(label) < max_label_len:\n",
    "                pad_label.extend([PAD] * (max_label_len-len(label)))\n",
    "                \n",
    "                \n",
    "            batch_x.append(pad_data)\n",
    "            batch_y.append(pad_label)\n",
    "            \n",
    "            # generate y_\n",
    "            focus_designate = [PAD] * len(pad_label)\n",
    "            focus_designate[0] = SOS\n",
    "            focus_designate[pad_label.index(EOS)] = EOS\n",
    "            \n",
    "            \n",
    "            idx1 = data[data.index('<EOS>')+2]\n",
    "            \n",
    "            \n",
    "            if data.index('<EOS>')+3 < len(data):\n",
    "                idx2 = data[data.index('<EOS>')+3]\n",
    "                idx = int(idx1) + int(idx2)\n",
    "            else:\n",
    "                idx = int(idx1)\n",
    "            focus_designate[idx] = pad_label[idx]\n",
    "            \n",
    "            batch_y_.append(focus_designate)\n",
    "            \n",
    "            \n",
    "        return torch.LongTensor(batch_x), torch.LongTensor(len_x), torch.LongTensor(batch_y), len_y, torch.LongTensor(batch_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence lenght: tensor([ 9, 12,  9, 11]) [9, 8, 13, 13] \n",
      "\n",
      "['<SOS>', '若', '心', '有', '城', '<EOS>', '伤', '1', '5', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '便', '不', '会', '感', '受', '伤', '疼', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '伤', '<PAD>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] \n",
      "\n",
      "['<SOS>', '那', '取', '名', '回', '忆', '的', '时', '光', '<EOS>', '下', '2']\n",
      "['<SOS>', '留', '下', '你', '我', '模', '样', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '<PAD>', '下', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] \n",
      "\n",
      "['<SOS>', '除', '非', '你', '说', '<EOS>', '憾', '4', '7', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '离', '开', '我', '你', '从', '不', '曾', '觉', '得', '遗', '憾', '<EOS>']\n",
      "['<SOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '憾', '<EOS>'] \n",
      "\n",
      "['<SOS>', '就', '是', '你', '爱', '的', '宽', '容', '<EOS>', '温', '5', '<PAD>']\n",
      "['<SOS>', '你', '眼', '底', '的', '温', '柔', '也', '为', '我', '保', '留', '<EOS>']\n",
      "['<SOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '温', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<EOS>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Validate~~~~\n",
    "\n",
    "dataset = SentDataset(train_set)\n",
    "dataloader = DataLoader(dataset=dataset,\n",
    "                        batch_size=4,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=dataset.collate_fn,\n",
    "                        num_workers=0)\n",
    "for x,x_len,y,y_len,y_ in dataloader:\n",
    "    print('Sentence lenght:',x_len,y_len,'\\n')\n",
    "    \n",
    "    for xi,yi,y_i in zip(x,y,y_):      \n",
    "        \n",
    "        print(embedder.unTokenize(xi))\n",
    "        print(embedder.unTokenize(yi))\n",
    "        print(embedder.unTokenize(y_i),'\\n')\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import  pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, output_size):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedder.get_dim())\n",
    "        self.embedding.weight = nn.Parameter(embedder.vectors)\n",
    "        self.gru = nn.GRU(embedding_size, output_size,batch_first=True, bias=False)\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        \n",
    "        # Sort mini-batch by input_lengths\n",
    "        sorted_input_lengths, indices = torch.sort(input_lengths,descending=True)\n",
    "        _, desorted_indices = torch.sort(indices, descending=False)\n",
    "        input_seqs = input_seqs[indices]\n",
    "        \n",
    "        # Encoder work\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = pack_padded_sequence(embedded, sorted_input_lengths.cpu().numpy(), batch_first=True)\n",
    "        packed_outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = pad_packed_sequence(packed_outputs,batch_first=True)\n",
    "        \n",
    "        # Desort mini-batch\n",
    "        outputs = outputs[desorted_indices]\n",
    "        hidden = hidden[:,desorted_indices]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, teacher_forcing_ratio=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(embedder.get_vocabulary_size(),embedder.get_dim()) # Unused\n",
    "        self.embedding.weight = nn.Parameter(embedder.vectors)\n",
    "        self.cell = nn.GRUCell(embedder.get_dim(), hidden_size, bias=False)\n",
    "        self.clf = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        \n",
    "        if hidden_size == embedder.vectors.T.shape[0]:\n",
    "            self.clf.weight = nn.Parameter(embedder.vectors)\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)  # work with NLLLoss\n",
    "\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "    def forward_step(self, inputs, hidden):\n",
    "        \n",
    "        # Unused\n",
    "        embedded = self.embedding(inputs)\n",
    "        # For research : all x to 0\n",
    "        embedded = torch.zeros_like(embedded)\n",
    "        \n",
    "        hidden = self.cell(embedded, hidden) # [B,Hidden_dim]\n",
    "        clf_output = self.clf(hidden) # [B,Output_dim]\n",
    "        output = self.log_softmax(clf_output)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, context_vector, target_vars, target_lengths):\n",
    "\n",
    "        batch_size = context_vector.shape[1]\n",
    "        \n",
    "        decoder_input = torch.LongTensor([SOS] * batch_size).to(device)\n",
    "        decoder_hidden = context_vector.squeeze(0)\n",
    "\n",
    "        if target_lengths is None:\n",
    "            max_target_length = 50\n",
    "        else:\n",
    "            max_target_length = max(target_lengths)\n",
    "        decoder_outputs = []\n",
    "        decoder_hiddens = []\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        \n",
    "        for t in range(max_target_length):    \n",
    "            \n",
    "            decoder_outputs_on_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_outputs_on_t)\n",
    "            decoder_hiddens.append(decoder_hidden)\n",
    "            \n",
    "            # Take input for next GRU iteration\n",
    "            if use_teacher_forcing :\n",
    "                decoder_input = target_vars[:,t]\n",
    "            else:\n",
    "                decoder_input = decoder_outputs_on_t.argmax(-1)\n",
    "            \n",
    "            # Early Stop when all predict <EOS> \n",
    "            if torch.all(decoder_input==EOS) and target_lengths is None and self.train() == False:\n",
    "                break\n",
    "            \n",
    "        # Stack output of each word at dimension 2\n",
    "        decoder_outputs = torch.stack(decoder_outputs,dim=2)\n",
    "        # Stack hidden of each timestep at dimension 1\n",
    "        decoder_hiddens = torch.stack(decoder_hiddens,dim=1)\n",
    "        \n",
    "        return decoder_outputs, decoder_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, input_seqs, input_lengths, target_seqs=None, target_lengths=None):\n",
    "        outputs, hidden = encoder(input_seqs, input_lengths)\n",
    "        outputs, hiddens = decoder(hidden, target_seqs, target_lengths)\n",
    "        return outputs,hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6575, 300)\n",
       "    (gru): GRU(300, 128, bias=False, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(6575, 300)\n",
       "    (cell): GRUCell(300, 128, bias=False)\n",
       "    (clf): Linear(in_features=128, out_features=6575, bias=False)\n",
       "    (log_softmax): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "context_dim = 128\n",
    "\n",
    "encoder = Encoder(embedder.get_vocabulary_size(),embedder.get_dim(),output_size=context_dim)\n",
    "decoder = Decoder(context_dim,embedder.get_vocabulary_size(),0.5)\n",
    "model = Seq2Seq(encoder,decoder)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def run_epoch(epoch,dataset,isTraining):\n",
    "    \n",
    "    if isTraining:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=0)\n",
    "    \n",
    "    if isTraining:\n",
    "        desc='Train {}'\n",
    "    else:\n",
    "        desc='Valid {}'\n",
    "    \n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader),desc=desc.format(epoch))\n",
    "    \n",
    "    loss=0\n",
    "    acc = 0\n",
    "    \n",
    "    for i,(x,x_len,y,y_len,y_) in trange:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_ = y_.to(device)\n",
    "        \n",
    "        # outputs : [b,emb,s] , hiddens : [b,s,hidden]\n",
    "        outputs,hiddens = model(x,x_len,y,y_len)\n",
    "        \n",
    "        idx = y_>2\n",
    "        tf_map = y_[idx] == outputs.argmax(1)[idx]\n",
    "        batch_acc = tf_map.sum().cpu().float().numpy()/len(tf_map)\n",
    "        acc += batch_acc\n",
    "        \n",
    "        batch_loss_all = criterion(outputs, y)\n",
    "        batch_loss_designated = criterion(outputs, y_)\n",
    "        batch_loss = (1-focus_ratio)*batch_loss_all + focus_ratio*batch_loss_designated\n",
    "        \n",
    "        if isTraining:\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        loss += batch_loss.item()\n",
    "        \n",
    "        trange.set_postfix({'loss':loss/(i+1),'accuracy':acc/(i+1)})\n",
    "        \n",
    "        if isTraining:\n",
    "            history_loss['train'].append(batch_loss.item())\n",
    "            history_acc['train'].append(batch_acc)\n",
    "        else:\n",
    "            history_loss['valid'].append(batch_loss.item())\n",
    "            history_acc['valid'].append(batch_acc)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Training\n",
    "\n",
    "\n",
    "dataset_all = SentDataset(all_set)\n",
    "dataset_train = SentDataset(train_set)\n",
    "dataset_valid = SentDataset(valid_set)\n",
    "\n",
    "criterion = torch.nn.NLLLoss(ignore_index=PAD, size_average=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "max_epoch = 20\n",
    "focus_ratio = 0.5\n",
    "\n",
    "history_loss = {'train':[],'valid':[]}\n",
    "history_acc = {'train':[],'valid':[]}\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    # Training\n",
    "    run_epoch(epoch,dataset=dataset_train,isTraining=True)\n",
    "    \n",
    "    # Validation\n",
    "    run_epoch(epoch,dataset=dataset_valid,isTraining=False)\n",
    "    \n",
    "    # Saving\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    torch.save(model.state_dict(), 'model/model.pkl.{}'.format(epoch))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot (Loss and acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "\n",
    "modes = ['train', 'valid']\n",
    "recs = [history_loss, history_acc]\n",
    "names = ['Loss', 'Accuracy']\n",
    "\n",
    "values = []\n",
    "for mode in modes:\n",
    "    v = []\n",
    "    for rec in recs:\n",
    "        v.append(rec[mode])\n",
    "    values.append(v)\n",
    " \n",
    "plt.figure(figsize=(32, 4))\n",
    "plt.subplots_adjust(left=0.02, right=0.999)\n",
    "for r, name in enumerate(names):\n",
    "    plt.subplot(1, len(recs), r+1)\n",
    "    for m in range(len(modes)):\n",
    "        plt.plot(values[m][r])\n",
    "    plt.title(name)\n",
    "    plt.legend(modes)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.show()\n",
    "#plt.savefig('figure.png', dpi=100)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test Data\n",
    "### Define test data dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        max_data_len = max([len(data) for data in datas])\n",
    "        batch_x = []\n",
    "        len_x = []\n",
    "        \n",
    "        for data in datas:\n",
    "            len_x.append(len(data))\n",
    "            pad_data = [embedder.to_index(w) for w in data]\n",
    "            if len(data) < max_data_len:\n",
    "                pad_data.extend([PAD] * (max_data_len-len(data)))\n",
    "            batch_x.append(pad_data)\n",
    "\n",
    "        return torch.LongTensor(batch_x), torch.LongTensor(len_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6575, 300)\n",
       "    (gru): GRU(300, 128, bias=False, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(6575, 300)\n",
       "    (cell): GRUCell(300, 128, bias=False)\n",
       "    (clf): Linear(in_features=128, out_features=6575, bias=False)\n",
       "    (log_softmax): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_pkl = '../pre-train/model.pkl.2-2-additional_without42&24'\n",
    "model.load_state_dict(torch.load(path_pkl))\n",
    "model.decoder.teacher_forcing_ratio = 0.0\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 做一個指定位置的context hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_spos = ['<SOS>', '让', '我', '拥', '有', '了', '恬', '静', '的', '<EOS>', '静','3','2']\n",
    "len_context = torch.LongTensor([len(context_spos)])\n",
    "context_spos = [embedder.to_index(w) for w in context_spos]\n",
    "context_spos = torch.LongTensor([context_spos]).to(device)\n",
    "context_spos = model.encoder(context_spos,len_context)[1] \n",
    "context_spos = context_spos.squeeze()# context hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_spos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def Deconstruction(model,x,x_len):\n",
    "    \n",
    "    # Encoder \n",
    "    encoder_hiddens, context = model.encoder(x,x_len)\n",
    "    context = context.squeeze(0)\n",
    "    \n",
    "    # replace cells in context hidden by context_spos\n",
    "    cells = []\n",
    "    for c in cells:\n",
    "        context[:,c] = context_spos[c]\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_hidden = context.squeeze(0)\n",
    "    \n",
    "    # Collection signal\n",
    "    decoder_outputs = []\n",
    "    decoder_hiddens = []\n",
    "    decoder_resetGates = []\n",
    "    decoder_updateGates = []\n",
    "    decoder_newGates = []\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # GRU Cell\n",
    "        gru = model.decoder.cell\n",
    "        \n",
    "        U_h = F.linear(decoder_hidden, gru.weight_hh)\n",
    "        Ur_h, Uz_h, Un_h = U_h.chunk(3, 1)\n",
    "        reset_gate = torch.sigmoid(Ur_h)\n",
    "        update_gate = torch.sigmoid(Uz_h)\n",
    "        new_gate = torch.tanh(reset_gate * Un_h)\n",
    "        decoder_hidden = new_gate + update_gate * (decoder_hidden - new_gate)\n",
    "        \n",
    "        # Classifier\n",
    "        clf_output = model.decoder.clf(decoder_hidden)\n",
    "        decoder_output = model.decoder.log_softmax(clf_output)\n",
    "        \n",
    "        decoder_resetGates.append(reset_gate)\n",
    "        decoder_updateGates.append(update_gate)\n",
    "        decoder_newGates.append(new_gate)\n",
    "        decoder_outputs.append(decoder_output)            \n",
    "        decoder_hiddens.append(decoder_hidden)\n",
    "                                                   \n",
    "        if torch.all(decoder_output.argmax(-1)==EOS) == True:\n",
    "            break\n",
    "            \n",
    "    outputs = torch.stack(decoder_outputs,dim=2)             # (b,6xxx,s)\n",
    "    \n",
    "    gru_info = { \n",
    "        'context':context,                                        # (b,128)\n",
    "        'hiddens':torch.stack(decoder_hiddens,dim=2),             # (b,128,s)\n",
    "        'resetgates':torch.stack(decoder_resetGates,dim=2),       # (b,128,s)\n",
    "        'updategates':torch.stack(decoder_updateGates,dim=2),     # (b,128,s)\n",
    "        'newgates':torch.stack(decoder_newGates,dim=2)            # (b,128,s)\n",
    "    }\n",
    "    \n",
    "    return outputs, gru_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate certain condition valid datas ( by designated word / position filter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', '重', '温', '几', '次', '啊', '啊', '啊', '啊', '<EOS>', '极', '4', '2']\n",
      "['<SOS>', '一', '千', '杯', '不', '醉', '不', '想', '睡', '<EOS>', '黑', '4', '2']\n",
      "['<SOS>', '让', '美', '好', '带', '来', '欢', '笑', '在', '<EOS>', '般', '4', '2']\n",
      "['<SOS>', '是', '否', '你', '让', '我', '受', '的', '不', '<EOS>', '中', '4', '2']\n",
      "['<SOS>', '为', '什', '么', '留', '下', '这', '个', '结', '<EOS>', '唉', '4', '2']\n",
      "['<SOS>', '有', '了', '你', '啊', '啊', '啊', '啊', '啊', '<EOS>', '意', '4', '2']\n",
      "['<SOS>', '我', '的', '舞', '台', '自', '己', '主', '宰', '<EOS>', '望', '4', '2']\n",
      "['<SOS>', '唱', '给', '那', '的', '小', '孩', '啊', '啊', '<EOS>', '只', '4', '2']\n",
      "['<SOS>', '夜', '山', '森', '木', '啊', '啊', '啊', '啊', '<EOS>', '爱', '4', '2']\n",
      "['<SOS>', '一', '一', '检', '点', '啊', '啊', '啊', '啊', '<EOS>', '命', '4', '2']\n",
      "['<SOS>', '我', '却', '不', '知', '不', '觉', '的', '啊', '<EOS>', '便', '4', '2']\n",
      "['<SOS>', '呀', '啦', '嘿', '啊', '啊', '啊', '啊', '啊', '<EOS>', '开', '4', '2']\n",
      "['<SOS>', '远', '的', '可', '以', '把', '过', '去', '遗', '<EOS>', '须', '4', '2']\n",
      "['<SOS>', '你', '或', '者', '一', '直', '觉', '得', '这', '<EOS>', '觉', '4', '2']\n",
      "['<SOS>', '对', '折', '再', '对', '折', '轻', '轻', '把', '<EOS>', '你', '4', '2']\n",
      "['<SOS>', '在', '我', '和', '你', '的', '痛', '苦', '中', '<EOS>', '埃', '4', '2']\n"
     ]
    }
   ],
   "source": [
    "certain_set = []\n",
    "\n",
    "for sent in random.sample(corpus, k=16):\n",
    "    \n",
    "    if len(sent) > 8:\n",
    "        sent = sent[:8]\n",
    "    else:\n",
    "        sent = sent + ['啊']*(8-len(sent))\n",
    "    \n",
    "    designated_POS = random.randint(a=2,b=10)\n",
    "    designated_POS1 = random.randint(a=1,b=designated_POS-1)\n",
    "    designated_POS2 = designated_POS - designated_POS1\n",
    "    \n",
    "    designated_word = random.choice(random.choice(corpus))\n",
    "    \n",
    "    control_signal = [ designated_word, str(designated_POS1) , str(designated_POS2)]\n",
    "    \n",
    "    control_signal = [ designated_word, '4','2']\n",
    "    \n",
    "    data = ['<SOS>'] + sent + ['<EOS>'] + control_signal\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    certain_set.append(data)\n",
    "    \n",
    "dataset_certain = TestDataset(certain_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction certain condition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b661de9a6dc4ed0834a0d3262ed3bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=dataset_certain,\n",
    "                        batch_size=128,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=dataset_certain.collate_fn,\n",
    "                        num_workers=0)\n",
    "\n",
    "predictions = []\n",
    "trange = tqdm(dataloader, total=len(dataloader))\n",
    "\n",
    "for x,x_len in trange:\n",
    "    \n",
    "    x = x.to(device)\n",
    "    \n",
    "    outputs,gru_info = Deconstruction(model,x,x_len)\n",
    "    \n",
    "    for pred in outputs.cpu().detach().numpy().argmax(1):\n",
    "        predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\t<SOS>重温几次啊啊啊啊<EOS>极42\n",
      "pred:\t<SOS>我是的的的极<EOS>\n",
      "\n",
      "input:\t<SOS>一千杯不醉不想睡<EOS>黑42\n",
      "pred:\t<SOS>我不的的的黑<EOS>\n",
      "\n",
      "input:\t<SOS>让美好带来欢笑在<EOS>般42\n",
      "pred:\t<SOS>我是的的的般<EOS>\n",
      "\n",
      "input:\t<SOS>是否你让我受的不<EOS>中42\n",
      "pred:\t<SOS>我在你在心中<EOS>\n",
      "\n",
      "input:\t<SOS>为什么留下这个结<EOS>唉42\n",
      "pred:\t<SOS>你哈你的唉唉<EOS>\n",
      "\n",
      "input:\t<SOS>有了你啊啊啊啊啊<EOS>意42\n",
      "pred:\t<SOS>我你你的的意<EOS>\n",
      "\n",
      "input:\t<SOS>我的舞台自己主宰<EOS>望42\n",
      "pred:\t<SOS>我在我的希望<EOS>\n",
      "\n",
      "input:\t<SOS>唱给那的小孩啊啊<EOS>只42\n",
      "pred:\t<SOS>我的的的我只是<EOS>\n",
      "\n",
      "input:\t<SOS>夜山森木啊啊啊啊<EOS>爱42\n",
      "pred:\t<SOS>我妹的的的爱<EOS>\n",
      "\n",
      "input:\t<SOS>一一检点啊啊啊啊<EOS>命42\n",
      "pred:\t<SOS>我是的的生命<EOS>\n",
      "\n",
      "input:\t<SOS>我却不知不觉的啊<EOS>便42\n",
      "pred:\t<SOS>我不的的我便<EOS>\n",
      "\n",
      "input:\t<SOS>呀啦嘿啊啊啊啊啊<EOS>开42\n",
      "pred:\t<SOS>我妹我我离开<EOS>\n",
      "\n",
      "input:\t<SOS>远的可以把过去遗<EOS>须42\n",
      "pred:\t<SOS>我不我不必须<EOS>\n",
      "\n",
      "input:\t<SOS>你或者一直觉得这<EOS>觉42\n",
      "pred:\t<SOS>我不我我感觉<EOS>\n",
      "\n",
      "input:\t<SOS>对折再对折轻轻把<EOS>你42\n",
      "pred:\t<SOS>我不我不爱你<EOS>\n",
      "\n",
      "input:\t<SOS>在我和你的痛苦中<EOS>埃42\n",
      "pred:\t<SOS>我你你的尘埃<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process multi-EOS tokens\n",
    "predictions_set = []\n",
    "for p in predictions:\n",
    "    p = list(p)\n",
    "    if EOS in p:\n",
    "        p = p[:p.index(EOS)+1]\n",
    "    else:\n",
    "        p.append(EOS)\n",
    "    predictions_set.append(embedder.unTokenize(p))\n",
    "    \n",
    "\n",
    "for sent_id in range(len(certain_set)):\n",
    "    sent_in = ''.join(certain_set[sent_id])\n",
    "    sent_pred = ''.join(predictions_set[sent_id])\n",
    "    print('input:\\t{}\\npred:\\t{}\\n'.format(sent_in,sent_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 9), (128, 9), (128, 9), (16, 128))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updategates = gru_info['updategates'].mean(0).detach().cpu().numpy()\n",
    "\n",
    "resetgates = gru_info['resetgates'].mean(0).detach().cpu().numpy()\n",
    "\n",
    "newgates = gru_info['newgates'].mean(0).detach().cpu().numpy()\n",
    "\n",
    "context = gru_info['context'].detach().cpu().numpy()\n",
    "\n",
    "output_len = updategates.shape[1]\n",
    "\n",
    "updategates.shape , resetgates.shape , newgates.shape , context.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## context_set[i] 收集position為i的輸入句經過Encoder產生的context(h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 极\n",
      "6 黑\n",
      "6 般\n",
      "6 中\n",
      "6 唉\n",
      "6 意\n",
      "6 望\n",
      "6 只\n",
      "6 爱\n",
      "6 命\n",
      "6 便\n",
      "6 开\n",
      "6 须\n",
      "6 觉\n",
      "6 你\n",
      "6 埃\n",
      "pos 1 is empty\n",
      "pos 2 is empty\n",
      "pos 3 is empty\n",
      "pos 4 is empty\n",
      "pos 5 is empty\n",
      "pos 7 is empty\n",
      "pos 8 is empty\n",
      "pos 9 is empty\n",
      "pos 10 is empty\n",
      "pos 11 is empty\n",
      "pos 12 is empty\n"
     ]
    }
   ],
   "source": [
    "context_set = [[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "for xi,hi in zip(x,context):\n",
    "    xi = xi.cpu().numpy()\n",
    "    pos = 0\n",
    "    for token in xi[::-1]:\n",
    "        w = embedder.to_word(token)\n",
    "        if w.isdigit():\n",
    "            pos += int(w)\n",
    "        else:\n",
    "            break\n",
    "    print(pos,w)\n",
    "    context_set[pos].append(hi)\n",
    "    \n",
    "for i in range(1,len(context_set)):\n",
    "    try:\n",
    "        context_set[i] = np.stack(context_set[i],axis=0)\n",
    "    except:\n",
    "        print('pos',i,'is empty')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
